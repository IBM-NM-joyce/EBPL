import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import cv2
import numpy as np

# ---------------------------
# STEP 1: Load & Preprocess Data
# ---------------------------
train_dir = "dataset/train"
test_dir = "dataset/test"

# Data Augmentation
datagen = ImageDataGenerator(rescale=1./255,
                             shear_range=0.2,
                             zoom_range=0.2,
                             horizontal_flip=True)

train_data = datagen.flow_from_directory(train_dir,
                                         target_size=(64, 64),
                                         batch_size=32,
                                         class_mode='categorical')

test_data = datagen.flow_from_directory(test_dir,
                                        target_size=(64, 64),
                                        batch_size=32,
                                        class_mode='categorical')

# ---------------------------
# STEP 2: Build CNN Model
# ---------------------------
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),
    MaxPooling2D(pool_size=(2,2)),

    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(train_data.num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# ---------------------------
# STEP 3: Train the Model
# ---------------------------
model.fit(train_data, epochs=10, validation_data=test_data)

# Save model
model.save("hand_gesture_model.h5")

# ---------------------------
# STEP 4: Real-Time Prediction (with Webcam)
# ---------------------------
gesture_dict = {i: gesture for gesture, i in train_data.class_indices.items()}

cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    roi = cv2.resize(frame, (64, 64))
    roi = roi / 255.0
    roi = np.expand_dims(roi, axis=0)

    prediction = model.predict(roi)
    gesture = gesture_dict[np.argmax(prediction)]

    cv2.putText(frame, f"Gesture: {gesture}", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)
    cv2.imshow("Hand Gesture Recognition", frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()